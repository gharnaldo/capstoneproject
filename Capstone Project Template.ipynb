{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "# Project Capstone\n",
    "### Data Engineering Capstone Project\n",
    "\n",
    "#### Project Summary\n",
    "This Project aims to provide useful information to analysts as well as for the immigration staff in order they can make decisions regarding immigration process because they will be able to access what is the current and historical situation about population who have entered the country, what is the relation with demographics indicators and also what has been the behavior regarding COVID19 in the country where visitors resides.\n",
    "\n",
    "The project follows the follow steps:\n",
    "* Step 1: Scope the Project and Gather Data\n",
    "* Step 2: Explore and Assess the Data\n",
    "* Step 3: Define the Data Model\n",
    "* Step 4: Run ETL to Model the Data\n",
    "* Step 5: Complete Project Write Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Do all imports and installs here\n",
    "import pandas as pd, re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 1: Scope the Project and Gather Data\n",
    "\n",
    "#### Scope \n",
    "This Project aims to provide useful information to analysts as well as for the immigration staff in order they can make decisions regarding immigration process because they will be able to access what is the current and historical situation about population who have entered the country, what is the relation with demographics indicators and also what has been the behavior regarding COVID19 in the country where visitors resides. For that we have built a Datapipeline by using pandas, spark with python. The information is taken from different sources, organize them into a model and written into parquet files.  \n",
    "\n",
    "#### Describe and Gather Data \n",
    "\n",
    "* Immigration data comes from the <a href=\"https://www.trade.gov/national-travel-and-tourism-office\">US National Tourism and Trade Office</a>\n",
    "* US cities demographics information is provided with the project. Likewise data of countries, us states and i94ports to ensure the data quality.\n",
    "* COVID19 by country is from <a href=\"https://www.kaggle.com/datasets/jcsantiago/covid19-by-country-with-government-response\">Kaggle</a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, levenshtein\n",
    "spark = SparkSession.builder.\\\n",
    "config(\"spark.jars.repositories\", \"https://repos.spark-packages.org/\").\\\n",
    "config(\"spark.jars.packages\", \"saurfang:spark-sas7bdat:2.0.0-s_2.11\").\\\n",
    "enableHiveSupport().getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 2: Explore and Assess the Data\n",
    "#### Explore the Data \n",
    "Identify data quality issues, like missing values, duplicate data, etc.\n",
    "\n",
    "#### Cleaning Steps\n",
    "Document steps necessary to clean the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Parses the file I94_SAS_Labels_Descriptions.SAS dinamically by iterating each line of each group of data within the file, discard the data that shouldn't be used, and then move the data into a dictionary and then into a pandas Dataframe.\n",
    "def build_sas_table(target,array_words,keytype):\n",
    "    array_valids = {}\n",
    "    with open('I94_SAS_Labels_Descriptions.SAS') as f:\n",
    "        start = False\n",
    "        for line in f:\n",
    "            if not start:\n",
    "                match_start = re.compile(r'%s$' %(target)).search(line)\n",
    "            if match_start:\n",
    "                start = True\n",
    "                match_end = re.compile(r';$').search(line)\n",
    "                if match_end:\n",
    "                    break\n",
    "                if keytype == 1:\n",
    "                    match = re.compile(r\"'([^=]*)'.*'(.*)'\").search(line)\n",
    "                else:\n",
    "                    match = re.compile(r\"([^=]*).*'(.*)'\").search(line)\n",
    "                if match:         \n",
    "                    i94port_invalid = False\n",
    "                    for i in array_words:             \n",
    "                        if re.search(i,match[2]):                       \n",
    "                            i94port_invalid = True\n",
    "                            break\n",
    "                    if not i94port_invalid:\n",
    "                        array_valids[match[1]]=match[2].strip()\n",
    "    return array_valids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract the data from I94_SAS_Labels_Descriptions.SAS to clean it and generate the Dataframe for dim_i94ports\n",
    "dfi94port = pd.DataFrame.from_dict(build_sas_table(\"i94prtl\",['No PORT','Collapsed'],1), orient='index', columns = {'i94port_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract the data from I94_SAS_Labels_Descriptions.SAS to clean it and generate the Dataframe for dim_countries\n",
    "country = pd.DataFrame.from_dict(build_sas_table(\"i94cntyl\",['INVALID','Collapsed','No Country'],2), orient='index', columns = {'country_name'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "# Extract the data from I94_SAS_Labels_Descriptions.SAS to clean it and generate the Dataframe for dim_us_states\n",
    "us_states = pd.DataFrame.from_dict(build_sas_table(\"i94addrl\",[],1), orient='index', columns = {'us_states_codes'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read the file covid19_by_country.csv\n",
    "covid19_by_country = pd.read_csv('covid19_by_country_small.csv', sep=',')\n",
    "covid19_by_country_table = spark.createDataFrame(covid19_by_country) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read the file us-cities-demographics.csv\n",
    "demographic = pd.read_csv('us-cities-demographics.csv', sep=';')\n",
    "demographic_table = spark.createDataFrame(demographic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Read the files from trade.gov\n",
    "df_immigration = spark.read.format('com.github.saurfang.sas.spark').load('/data/18-83510-I94-Data-2016/i94_apr16_sub.sas7bdat')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 3: Data Model\n",
    "#### 3.1 Conceptual Data Model\n",
    "\n",
    "<img src=\"../assets/ER_diagram_updated.png\" width=\"1250\"/>\n",
    "\n",
    "#### 3.2 Mapping Out Data Pipelines\n",
    "\n",
    "* Given the file I94_SAS_Labels_Descriptions.SAS is in the project's root directory, likewise the different sources (covid19_by_country.csv, us-cities-demographics.csv)\n",
    "* Parse the file I94_SAS_Labels_Descriptions.SAS dinamically by iterating each line of each group of data within the file, clean the data, and then move it into a dictionary and then into a pandas Dataframe.\n",
    "* Generate the TempView for dim_i94ports table. \n",
    "* Generate the TempView for dim_countries table.\n",
    "* Generate the TempView for dim_us_states table.\n",
    "* Read the file covid19_by_country.csv and then generate the TempView for covid_by_country table, grouping the data by country, year, month and leaving the most recient number of active cases.\n",
    "* Compare by applying Levenshtein and spark algorythm for set the threecountrycode in the countries_table. Add that new column to dim_countries table.\n",
    "* Read the file us-cities-demographics.csv and then generate the TempView for dim_demographic table, grouping the data by us_state and summing the foreignborn, total population, population by gender and average population ages.\n",
    "* Read the folder where the immigration data is, clean the data by joining on dim i94ports table and dim us_states table.\n",
    "* Generate the TemView for immigrations table.\n",
    "* Write the parquet files for the tables: dim i94ports, dim countries, dim us_states, dim demographic, covid_by_country and immigration.\n",
    "* Run quality checks to ensure the data were loaded properly and schemas are correct. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "### Step 4: Run Pipelines to Model the Data \n",
    "#### 4.1 Create the data model\n",
    "Build the data pipelines to create the data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+--------------------+\n",
      "|level_0|index|        i94port_name|\n",
      "+-------+-----+--------------------+\n",
      "|      0|  ALC|           ALCAN, AK|\n",
      "|      1|  ANC|       ANCHORAGE, AK|\n",
      "|      2|  BAR|BAKER AAF - BAKER...|\n",
      "+-------+-----+--------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+------------------+-----+------------+\n",
      "|summary|           level_0|index|i94port_name|\n",
      "+-------+------------------+-----+------------+\n",
      "|  count|               588|  588|         588|\n",
      "|   mean|             293.5|888.0|        null|\n",
      "| stddev|169.88525539316237|  NaN|        null|\n",
      "|    min|                 0|  48Y|ABERDEEN, WA|\n",
      "|    max|               587|  ZZZ|    YUMA, AZ|\n",
      "+-------+------------------+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate TempView for dim_i94ports table\n",
    "dfi94port.reset_index(drop=False,inplace=True)\n",
    "i94ports_table = spark.createDataFrame(dfi94port) \n",
    "i94ports_table.createOrReplaceTempView(\"i94ports_table\")\n",
    "i94ports_table.show(3)\n",
    "i94ports_table.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------+------------------+\n",
      "|  index|country_name|country_name_lower|\n",
      "+-------+------------+------------------+\n",
      "|   582 |      MEXICO|            mexico|\n",
      "|   236 | AFGHANISTAN|       afghanistan|\n",
      "|   101 |     ALBANIA|           albania|\n",
      "+-------+------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+------------------+------------+------------------+\n",
      "|summary|             index|country_name|country_name_lower|\n",
      "+-------+------------------+------------+------------------+\n",
      "|  count|               236|         236|               236|\n",
      "|   mean| 362.5550847457627|        null|              null|\n",
      "| stddev|187.97711816271342|        null|              null|\n",
      "|    min|              101 | AFGHANISTAN|       afghanistan|\n",
      "|    max|              760 |    ZIMBABWE|          zimbabwe|\n",
      "+-------+------------------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate TempView for dim_countries table\n",
    "country.reset_index(drop=False,inplace=True)\n",
    "countries_table = spark.createDataFrame(country)\n",
    "countries_table = countries_table.selectExpr(\"index\",\"country_name\",\"LOWER(country_name) AS country_name_lower\")\n",
    "#countries_table.createOrReplaceTempView(\"countries_table\")\n",
    "countries_table.show(3)\n",
    "countries_table.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+---------------+\n",
      "|level_0|index|us_states_codes|\n",
      "+-------+-----+---------------+\n",
      "|      0|   AL|        ALABAMA|\n",
      "|      1|   AK|         ALASKA|\n",
      "|      2|   AZ|        ARIZONA|\n",
      "+-------+-----+---------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+------------------+-----+---------------+\n",
      "|summary|           level_0|index|us_states_codes|\n",
      "+-------+------------------+-----+---------------+\n",
      "|  count|                54|   54|             54|\n",
      "|   mean|              26.5| null|           null|\n",
      "| stddev|15.732132722552274| null|           null|\n",
      "|    min|                 0|   AK|        ALABAMA|\n",
      "|    max|                53|   WY|        WYOMING|\n",
      "+-------+------------------+-----+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate TempView for dim_us_states table\n",
    "us_states.reset_index(drop=False,inplace=True)\n",
    "us_states_table = spark.createDataFrame(us_states) \n",
    "us_states_table.createOrReplaceTempView(\"us_states_table\")\n",
    "us_states_table.show(3)\n",
    "us_states_table.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+----+-----+---------+-------------+\n",
      "|countryalpha3code|year|month|  country|confirmed_inc|\n",
      "+-----------------+----+-----+---------+-------------+\n",
      "|              AUS|2020|    6|Australia|         86.0|\n",
      "|              AUT|2021|    7|  Austria|        538.0|\n",
      "|              BHR|2021|    1|  Bahrain|        431.0|\n",
      "+-----------------+----+-----+---------+-------------+\n",
      "only showing top 3 rows\n",
      "\n",
      "+-------+-----------------+------------------+-----------------+-----------+------------------+\n",
      "|summary|countryalpha3code|              year|            month|    country|     confirmed_inc|\n",
      "+-------+-----------------+------------------+-----------------+-----------+------------------+\n",
      "|  count|              770|               770|              770|        770|               770|\n",
      "|   mean|             null|2020.4233766233767|5.855844155844156|       null|1563.2805194805194|\n",
      "| stddev|             null|0.4944151301378696|3.207395286996354|       null| 6619.320648358399|\n",
      "|    min|              AFG|              2020|                1|Afghanistan|               0.0|\n",
      "|    25%|             null|              2020|                3|       null|               0.0|\n",
      "|    50%|             null|              2020|                6|       null|              36.0|\n",
      "|    75%|             null|              2021|                8|       null|             524.0|\n",
      "|    max|              TCD|              2021|               12|      China|           90638.0|\n",
      "+-------+-----------------+------------------+-----------------+-----------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate TempView for dim_covid_by_country table grouped by country\n",
    "def generate_covid19_table_grouped(table):\n",
    "    covid19_table_grouped = table.selectExpr(\"Country\", \"confirmed_inc\", \"year(Date) AS year\", \"month(Date) AS month\", \"countryalpha3code\") \\\n",
    "                        .groupBy(\"countryalpha3code\",\"year\",\"month\") \\\n",
    "                        .agg({\"confirmed_inc\":\"last\",\"Country\":\"last\"}) \\\n",
    "                        .withColumnRenamed('last(year)','year') \\\n",
    "                        .withColumnRenamed('last(confirmed_inc)','confirmed_inc') \\\n",
    "                        .withColumnRenamed('last(Country)','country')\n",
    "    covid19_table_grouped.createOrReplaceTempView(\"covid19_table_grouped\")\n",
    "    return covid19_table_grouped\n",
    "covid19_table_grouped = generate_covid19_table_grouped(covid19_by_country_table)\n",
    "covid19_table_grouped.show(3)\n",
    "covid19_table_grouped.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+--------+-------------+\n",
      "|countryalpha3code| country|country_lower|\n",
      "+-----------------+--------+-------------+\n",
      "|              BRB|Barbados|     barbados|\n",
      "|              BRA|  Brazil|       brazil|\n",
      "|              ARM| Armenia|      armenia|\n",
      "+-----------------+--------+-------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "37"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Generate temporal dataframe for dim_covid_by_country country names\n",
    "def generate_countries_from_covid19_df(table):\n",
    "    countries_from_covid19_df = table.selectExpr(\"countryalpha3code\", \"country\", \"LOWER(country) AS lower_country\") \\\n",
    "                        .groupBy(\"countryalpha3code\") \\\n",
    "                        .agg({\"country\":\"last\", \"lower_country\":\"last\"}) \\\n",
    "                        .withColumnRenamed('last(country)','country') \\\n",
    "                        .withColumnRenamed('last(lower_country)','country_lower')\n",
    "    return countries_from_covid19_df\n",
    "countries_from_covid19_df = generate_countries_from_covid19_df(covid19_table_grouped)\n",
    "countries_from_covid19_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------+--------+\n",
      "|countryalpha3code| i94res| country|\n",
      "+-----------------+-------+--------+\n",
      "|              BRB|   513 |Barbados|\n",
      "|              BRA|   689 |  Brazil|\n",
      "|              ARM|   151 | Armenia|\n",
      "+-----------------+-------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Merge country table and covid19_by_country, then apply levenshtein algorythm to link i94res and countryalpha3code fields\n",
    "merged_data=countries_from_covid19_df.crossJoin(countries_table)\n",
    "merged_data = merged_data.withColumn(\"levenshtein\", levenshtein(col(\"country_name_lower\"), col(\"country_lower\")))\n",
    "merged_data = merged_data.filter(\"levenshtein < 2\")\n",
    "countries_table = merged_data.selectExpr(\"countryalpha3code\",\"index as i94res\",\"country\")\n",
    "countries_table.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------------+----------------+-----------------+-----------+--------------+\n",
      "|statecode|totalpopulation|femalepopulation|        medianage|foreignborn|malepopulation|\n",
      "+---------+---------------+----------------+-----------------+-----------+--------------+\n",
      "|       AZ|       22497710|     1.1360435E7|35.03750000000001|  3411565.0|   1.1137275E7|\n",
      "|       SC|        2586976|       1321685.0|33.82500000000001|   134019.0|     1265291.0|\n",
      "|       LA|        6502975|       3367985.0|34.62500000000001|   417095.0|     3134990.0|\n",
      "+---------+---------------+----------------+-----------------+-----------+--------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate TempView for dim_demographic table grouped by country\n",
    "def generate_demographic_table_grouped(table):\n",
    "    demographic_table_grouped = table.select(\"State Code\",\"Foreign-born\",\"Total Population\",\"Median Age\", \"Male Population\", \"Female Population\") \\\n",
    "                        .groupBy(col(\"State Code\").alias(\"statecode\")) \\\n",
    "                        .agg({\"Foreign-born\":\"sum\",\"Total Population\":\"sum\",\"Median Age\":\"mean\",\"Male Population\":\"sum\",\"Female Population\":\"sum\"}) \\\n",
    "                        .withColumnRenamed('sum(Foreign-born)','foreignborn') \\\n",
    "                        .withColumnRenamed('sum(Total Population)','totalpopulation') \\\n",
    "                        .withColumnRenamed('sum(Male Population)','malepopulation') \\\n",
    "                        .withColumnRenamed('sum(Female Population)','femalepopulation') \\\n",
    "                        .withColumnRenamed('avg(Median Age)','medianage') \n",
    "    demographic_table_grouped.createOrReplaceTempView(\"demographic_table_grouped\")\n",
    "    return demographic_table_grouped\n",
    "demographic_table_grouped = generate_demographic_table_grouped(demographic_table)\n",
    "demographic_table_grouped.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------+------+-------+-------+-------+-------+------+-------+-----+-------+\n",
      "| i94yr|i94mon|i94res|i94port|i94addr|arrdate|depdate|gender|biryear|fltno|i94visa|\n",
      "+------+------+------+-------+-------+-------+-------+------+-------+-----+-------+\n",
      "|2016.0|   4.0| 509.0|    FMY|     AZ|20545.0|20595.0|     M| 1962.0|01773|    2.0|\n",
      "|2016.0|   4.0| 135.0|    FMY|     AZ|20546.0|20555.0|     M| 2001.0|00137|    2.0|\n",
      "|2016.0|   4.0| 135.0|    FMY|     AZ|20546.0|20579.0|     M| 1948.0|00127|    2.0|\n",
      "+------+------+------+-------+-------+-------+-------+------+-------+-----+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Generate the TempView for dim_immigration table joining on dim_countries, dim_i94ports, dim_us_states and dim_demographic \n",
    "def generate_immigration_table_temp(table):    \n",
    "    immigration_table_temp = spark.sql(\"\"\"\n",
    "        SELECT i94yr, i94mon, i94res, i94port, i94addr, arrdate, depdate, gender, biryear, fltno, i94visa\n",
    "        FROM immigration_table_temp\n",
    "        JOIN i94ports_table ON (immigration_table_temp.i94port = i94ports_table.index)\n",
    "        JOIN us_states_table ON (immigration_table_temp.i94addr = us_states_table.index)\n",
    "        \"\"\")\n",
    "    immigration_table_temp.createOrReplaceTempView(table)\n",
    "    return immigration_table_temp\n",
    "df_immigration.createOrReplaceTempView(\"immigration_table_temp\")\n",
    "immigration_table_temp = generate_immigration_table_temp(\"immigration_table_temp\")\n",
    "immigration_table_temp.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write dim_i94ports into parquet file\n",
    "i94ports_table.write.mode('overwrite').parquet(\"output_data/dim_i94ports/dim_i94ports.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write dim_countries into parquet file\n",
    "countries_table.write.mode('overwrite').parquet(\"output_data/dim_countries/dim_countries.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write dim_us_states into parquet file\n",
    "us_states_table.write.mode('overwrite').parquet(\"output_data/dim_us_states/dim_us_states.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write dim_covid_by_country into parquet file\n",
    "covid19_table_grouped.write.mode('overwrite').partitionBy(\"country\").parquet(\"output_data/dim_covid_by_country/dim_covid_by_country.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write dim_demographic into parquet file\n",
    "demographic_table_grouped.write.mode('overwrite').parquet(\"output_data/dim_demographic/dim_demographic.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Write dim_immigration into parquet file\n",
    "immigration_table_temp.write.mode('overwrite').partitionBy(\"i94addr\").parquet(\"output_data/dim_immigration/dim_immigration.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.2 Data Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Checks number of records is greater than 0\n",
    "def verify_not_empty(df):\n",
    "    num_rows = df.count()\n",
    "    assert(num_rows > 0)\n",
    "    \n",
    "# Perform quality checks here\n",
    "verify_not_empty(i94ports_table) \n",
    "verify_not_empty(countries_table) \n",
    "verify_not_empty(us_states_table) \n",
    "verify_not_empty(covid19_table_grouped) \n",
    "verify_not_empty(demographic_table_grouped) \n",
    "verify_not_empty(immigration_table_temp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": [
    "#Checks number of columns of each Dataframe\n",
    "def verify_num_cols(df,number):\n",
    "    num_cols = len(df.columns)\n",
    "    assert(num_cols == number)\n",
    "\n",
    "# Perform quality checks here\n",
    "verify_num_cols(i94ports_table,2) \n",
    "verify_num_cols(countries_table,3) \n",
    "verify_num_cols(us_states_table,2) \n",
    "verify_num_cols(covid19_table_grouped,5) \n",
    "verify_num_cols(demographic_table_grouped,6) \n",
    "verify_num_cols(immigration_table_temp,11) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### 4.3 Data dictionary \n",
    "\n",
    "dim_demographic\n",
    "* statecode: US States code\n",
    "* medianage: Population average age in that US State\n",
    "* totalpopulation: Total population in that US State\n",
    "* malepopulation: Total population of male gender in that US State\n",
    "* malepopulation: Total population of female gender in that US State\n",
    "* foreignborn: Foreign born population in that specific US State\n",
    "\n",
    "dim_immigration\n",
    "* i94yr: 4 digits year\n",
    "* i94mon: Numeric month\n",
    "* i94port: Destination city\n",
    "* i94res: Country code where the immigrant resides\n",
    "* i94addr: US State\n",
    "* arrdate: Date of arrival to US\n",
    "* depdate: Date to depart from US\n",
    "* gender: Gender\n",
    "* biryear: Year of birth\n",
    "* fitno: Flight number\n",
    "* i94visa: Category of Visa (1 = Business, 2 = Pleasure, 3 = Student)\n",
    "\n",
    "dim_i94ports\n",
    "* index: Destination city code\n",
    "* i94port: Destination city name\n",
    "\n",
    "dim_countries\n",
    "* i94res: Country code where the immigrant resides\n",
    "* country: Country name\n",
    "* countryaplpha3code: International Alpha 3 code for countries\n",
    "\n",
    "dim_us_states\n",
    "* index: US State code\n",
    "* country_name: US State name\n",
    "\n",
    "dim_covid19_by_country\n",
    "* country: Country name\n",
    "* date: Date of statistics\n",
    "* confirmed_inc: Confirmed cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "editable": true
   },
   "source": [
    "#### Step 5: Project Write Up\n",
    "* Data should be updated monthly because trade.gov posts the information in that frecuency. COVID19 data is posted daily and should be updated in the same frecuency. Demographic information could by updated yearly.\n",
    "* Mainly the selected tool has been Spark implemented with Python because it is robust, powerful and easy to integrate, also oriented to handling large volumes of data. In addition, it allows you to use a set of libraries such as Dataframes, SQL, algorithms such as Levenshtein's, etc. You can also scale up towards an EMR in AWS. The idea is to write the Parquet files in a AWS S3 bucket and then be extracted through the Data Pipeline to a AWS Redshift Database, which can be managed by end users through an API or Backend.\n",
    "* A Star Schema has been used for modeling the data with a slight mix with a Snowflake Schema, also known as Starflake Schema. This is due to the characteristics of the information that comes in the COVID19 table. You can either modify the information during the execution of the Pipeline or to set a relation between the covid19 table and the country table to solve that. The Star Schema allows you to make queries easiers with simple joins, furthermore aggegrations functions are lighter when you carry them out.  \n",
    "* Airflow would be a great and the suitable solution to implement the Pipeline calling the same funcions written here in a Operator which are able to be called from an Airflow schedule.\n",
    "* Description of how the project would approach the problem differently under the following scenarios:\n",
    " * If the data was increased by 100x is recommendable to move the Pipeline to a AWS EMR.\n",
    " * If the data populates a dashboard that must be updated on a daily basis by 7am every day is recommendable to set up Airflow Schedule and implement the same code in Operators to do that in the correct fashion.\n",
    " * If the database needed to be accessed by 100+ people is recommendable to populate a AWS Redshift with the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "editable": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
